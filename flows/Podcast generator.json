{"id":"2935e97f-7f7f-4098-8ff4-7276eabfe762","data":{"nodes":[{"id":"MergeDataComponent-wrxRM","type":"genericNode","position":{"x":908.4260810129742,"y":884.9964069299267},"data":{"type":"MergeDataComponent","node":{"template":{"_type":"Component","data_inputs":{"trace_as_metadata":true,"list":true,"trace_as_input":true,"required":false,"placeholder":"","show":true,"name":"data_inputs","value":"","display_name":"Data Inputs","advanced":false,"input_types":["Data"],"dynamic":false,"info":"List of Data objects to be merged.","title_case":false,"type":"other","_input_type":"DataInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import List, Any\nfrom loguru import logger\nfrom langflow.custom import Component\nfrom langflow.io import DataInput, Output\nfrom langflow.schema import Data\n\nclass MergeDataComponent(Component):\n    display_name = \"Merge Data\"\n    description = \"Combines multiple Data objects into a single unified Data object, preserving lists and handling duplicates.\"\n    icon = \"merge\"\n    beta = False\n\n    inputs = [\n        DataInput(\n            name=\"data_inputs\",\n            display_name=\"Data Inputs\",\n            info=\"List of Data objects to be merged.\",\n            is_list=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Merged Data\", name=\"merged_data\", method=\"merge_data\"),\n    ]\n\n    def _merge_data(self, target: dict, source: Any, key: str) -> None:\n        if key not in target:\n            target[key] = source\n        else:\n            if isinstance(target[key], list):\n                if isinstance(source, list):\n                    target[key].extend(source)\n                else:\n                    target[key].append(source)\n            elif isinstance(target[key], dict) and isinstance(source, dict):\n                for k, v in source.items():\n                    self._merge_data(target[key], v, k)\n            else:\n                target[key] = [target[key], source]\n\n    def merge_data(self) -> Data:\n        logger.info(\"Starting data merge process\")\n        data_inputs: List[Data] = self.data_inputs\n\n        if not data_inputs:\n            logger.warning(\"No data inputs provided\")\n            return Data()\n\n        if len(data_inputs) == 1:\n            logger.info(\"Only one data input provided, returning as is\")\n            return data_inputs[0]\n\n        merged_data = {}\n        for data in data_inputs:\n            logger.debug(f\"Merging data input\")\n            if not isinstance(data, Data):\n                logger.error(\"Input is not a Data object\")\n                raise ValueError(\"Input is not a Data object\")\n            \n            if data.data is None:\n                continue\n            \n            if isinstance(data.data, dict):\n                for key, value in data.data.items():\n                    self._merge_data(merged_data, value, key)\n            elif isinstance(data.data, list):\n                self._merge_data(merged_data, data.data, 'text')\n            else:\n                self._merge_data(merged_data, data.data, 'text')\n\n        logger.info(\"Data merge completed successfully\")\n        return Data(data=merged_data)","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false}},"description":"Combines multiple Data objects into a single unified Data object, preserving lists and handling duplicates.","icon":"merge","base_classes":["Data"],"display_name":"Merge Data","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":true,"outputs":[{"types":["Data"],"selected":"Data","name":"merged_data","display_name":"Merged Data","method":"merge_data","value":"__UNDEFINED__","cache":true}],"field_order":["data_inputs"],"beta":false,"edited":true,"lf_version":"1.0.19.post2"},"id":"MergeDataComponent-wrxRM"},"selected":false,"width":384,"height":283},{"id":"File-55Dzi","type":"genericNode","position":{"x":68,"y":902.2087964752402},"data":{"type":"File","node":{"template":{"_type":"Component","path":{"trace_as_metadata":true,"file_path":"2935e97f-7f7f-4098-8ff4-7276eabfe762/2024-10-29_18-54-50_factsheet-ai-ethics.txt","fileTypes":["txt","md","mdx","csv","json","yaml","yml","xml","html","htm","pdf","docx","py","sh","sql","js","ts","tsx"],"list":false,"required":false,"placeholder":"","show":true,"name":"path","value":"","display_name":"Path","advanced":false,"dynamic":false,"info":"Supported file types: txt, md, mdx, csv, json, yaml, yml, xml, html, htm, pdf, docx, py, sh, sql, js, ts, tsx","title_case":false,"type":"file","_input_type":"FileInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from pathlib import Path\n\nfrom langflow.base.data.utils import TEXT_FILE_TYPES, parse_text_file_to_data\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, FileInput, Output\nfrom langflow.schema import Data\n\n\nclass FileComponent(Component):\n    display_name = \"File\"\n    description = \"A generic file loader.\"\n    icon = \"file-text\"\n    name = \"File\"\n\n    inputs = [\n        FileInput(\n            name=\"path\",\n            display_name=\"Path\",\n            file_types=TEXT_FILE_TYPES,\n            info=f\"Supported file types: {', '.join(TEXT_FILE_TYPES)}\",\n        ),\n        BoolInput(\n            name=\"silent_errors\",\n            display_name=\"Silent Errors\",\n            advanced=True,\n            info=\"If true, errors will not raise an exception.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"data\", method=\"load_file\"),\n    ]\n\n    def load_file(self) -> Data:\n        if not self.path:\n            msg = \"Please, upload a file to use this component.\"\n            raise ValueError(msg)\n        resolved_path = self.resolve_path(self.path)\n        silent_errors = self.silent_errors\n\n        extension = Path(resolved_path).suffix[1:].lower()\n\n        if extension == \"doc\":\n            msg = \"doc files are not supported. Please save as .docx\"\n            raise ValueError(msg)\n        if extension not in TEXT_FILE_TYPES:\n            msg = f\"Unsupported file type: {extension}\"\n            raise ValueError(msg)\n\n        data = parse_text_file_to_data(resolved_path, silent_errors)\n        self.status = data or \"No data\"\n        return data or Data()\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"silent_errors":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"silent_errors","value":false,"display_name":"Silent Errors","advanced":true,"dynamic":false,"info":"If true, errors will not raise an exception.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"A generic file loader.","icon":"file-text","base_classes":["Data"],"display_name":"File","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"data","display_name":"Data","method":"load_file","value":"__UNDEFINED__","cache":true}],"field_order":["path","silent_errors"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19.post2"},"id":"File-55Dzi","description":"A generic file loader.","display_name":"File"},"selected":false,"width":384,"height":289,"dragging":false},{"id":"SplitText-DsAr8","type":"genericNode","position":{"x":485.4345283755724,"y":740.1313838628471},"data":{"type":"SplitText","node":{"template":{"_type":"Component","data_inputs":{"trace_as_metadata":true,"list":true,"required":false,"placeholder":"","show":true,"name":"data_inputs","value":"","display_name":"Data Inputs","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The data to split.","title_case":false,"type":"other","_input_type":"HandleInput"},"chunk_overlap":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"chunk_overlap","value":200,"display_name":"Chunk Overlap","advanced":false,"dynamic":false,"info":"Number of characters to overlap between chunks.","title_case":false,"type":"int","_input_type":"IntInput"},"chunk_size":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"chunk_size","value":1000,"display_name":"Chunk Size","advanced":false,"dynamic":false,"info":"The maximum number of characters in each chunk.","title_case":false,"type":"int","_input_type":"IntInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain_text_splitters import CharacterTextSplitter\n\nfrom langflow.custom import Component\nfrom langflow.io import HandleInput, IntInput, MessageTextInput, Output\nfrom langflow.schema import Data\nfrom langflow.utils.util import unescape_string\n\n\nclass SplitTextComponent(Component):\n    display_name: str = \"Split Text\"\n    description: str = \"Split text into chunks based on specified criteria.\"\n    icon = \"scissors-line-dashed\"\n    name = \"SplitText\"\n\n    inputs = [\n        HandleInput(\n            name=\"data_inputs\",\n            display_name=\"Data Inputs\",\n            info=\"The data to split.\",\n            input_types=[\"Data\"],\n            is_list=True,\n        ),\n        IntInput(\n            name=\"chunk_overlap\",\n            display_name=\"Chunk Overlap\",\n            info=\"Number of characters to overlap between chunks.\",\n            value=200,\n        ),\n        IntInput(\n            name=\"chunk_size\",\n            display_name=\"Chunk Size\",\n            info=\"The maximum number of characters in each chunk.\",\n            value=1000,\n        ),\n        MessageTextInput(\n            name=\"separator\",\n            display_name=\"Separator\",\n            info=\"The character to split on. Defaults to newline.\",\n            value=\"\\n\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Chunks\", name=\"chunks\", method=\"split_text\"),\n    ]\n\n    def _docs_to_data(self, docs):\n        return [Data(text=doc.page_content, data=doc.metadata) for doc in docs]\n\n    def split_text(self) -> list[Data]:\n        separator = unescape_string(self.separator)\n\n        documents = [_input.to_lc_document() for _input in self.data_inputs if isinstance(_input, Data)]\n\n        splitter = CharacterTextSplitter(\n            chunk_overlap=self.chunk_overlap,\n            chunk_size=self.chunk_size,\n            separator=separator,\n        )\n        docs = splitter.split_documents(documents)\n        data = self._docs_to_data(docs)\n        self.status = data\n        return data\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"separator":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"separator","value":"\n","display_name":"Separator","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The character to split on. Defaults to newline.","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Split text into chunks based on specified criteria.","icon":"scissors-line-dashed","base_classes":["Data"],"display_name":"Split Text","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"chunks","display_name":"Chunks","method":"split_text","value":"__UNDEFINED__","cache":true}],"field_order":["data_inputs","chunk_overlap","chunk_size","separator"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19.post2"},"id":"SplitText-DsAr8","description":"Split text into chunks based on specified criteria.","display_name":"Split Text"},"selected":false,"width":384,"height":525},{"id":"Prompt-n1kOC","type":"genericNode","position":{"x":1803.3419340224225,"y":692.9479285881691},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"# LLM Prompt: Generate Detailed Podcast Script\n\nUsing the provided data: {data}, create a comprehensive podcast script featuring {number_of_speakers} speakers, Person A, Person B, ..., with a total duration of {PodcastDuration}. \nIn this script you must separate the themes in a really amazing way, knowing that these parts will be recorded separatedly so they must include a good deep of details. \n","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput"},"PodcastDuration":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"PodcastDuration","display_name":"PodcastDuration","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"data":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"data","display_name":"data","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"number_of_speakers":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"number_of_speakers","display_name":"number_of_speakers","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["data","number_of_speakers","PodcastDuration"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":true,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true,"required_inputs":null}],"field_order":["template"],"beta":false,"error":null,"edited":false,"metadata":{},"lf_version":"1.0.19.post2"},"id":"Prompt-n1kOC"},"selected":false,"width":384,"height":563},{"id":"ParseData-2Umhs","type":"genericNode","position":{"x":1329.0962898199227,"y":866.5558052943995},"data":{"type":"ParseData","node":{"template":{"_type":"Component","data":{"trace_as_metadata":true,"list":false,"trace_as_input":true,"required":false,"placeholder":"","show":true,"name":"data","value":"","display_name":"Data","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The data to convert to text.","title_case":false,"type":"other","_input_type":"DataInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import DataInput, MultilineInput, Output, StrInput\nfrom langflow.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n    name = \"ParseData\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. \"\n            \"It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"sep":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"sep","value":"\n","display_name":"Separator","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"template":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"{text}","display_name":"Template","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Convert Data into plain text following a specified template.","icon":"braces","base_classes":["Message"],"display_name":"Parse Data","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"parse_data","value":"__UNDEFINED__","cache":true}],"field_order":["data","template","sep"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19.post2"},"id":"ParseData-2Umhs","description":"Convert Data into plain text following a specified template.","display_name":"Parse Data"},"selected":false,"width":384,"height":353},{"id":"TextInput-R3tNw","type":"genericNode","position":{"x":1455.1791424060395,"y":732.7451133234222},"data":{"type":"TextInput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"2 minutes","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as input.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Get text inputs from the Playground.","icon":"type","base_classes":["Message"],"display_name":"PodcastDuration","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":false,"lf_version":"1.0.18"},"id":"TextInput-R3tNw","showNode":false},"selected":false,"width":96,"height":96,"dragging":false,"positionAbsolute":{"x":1455.1791424060395,"y":732.7451133234222}},{"id":"OpenAIModel-0Us0w","type":"genericNode","position":{"x":4012.1610938378226,"y":243.86314362999337},"data":{"type":"OpenAIModel","node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"name":"api_key","value":"","display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    SecretStrInput,\n    StrInput,\n)\nfrom langflow.inputs.inputs import HandleInput\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. \"\n            \"You must pass the word JSON in the prompt. \"\n            \"If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. \"\n            \"Defaults to https://api.openai.com/v1. \"\n            \"You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        api_key = SecretStr(openai_api_key) if openai_api_key else None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})\n\n        return output\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"json_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"json_mode","value":false,"display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool","_input_type":"BoolInput"},"max_tokens":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"model_kwargs","value":{},"display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"dict","_input_type":"DictInput"},"model_name":{"trace_as_metadata":true,"options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"gpt-4o","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput","load_from_db":false},"openai_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"openai_api_base","value":"","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput"},"output_schema":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"name":"output_schema","value":{},"display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.","title_case":false,"type":"dict","_input_type":"DictInput"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"seed","value":1,"display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0.1,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generates text using OpenAI LLMs.","icon":"OpenAI","base_classes":["LanguageModel","Message"],"display_name":"OpenAI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":["input_value","stream","system_message"]},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":["api_key","json_mode","max_tokens","model_kwargs","model_name","openai_api_base","output_schema","seed","temperature"]}],"field_order":["input_value","system_message","stream","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","api_key","temperature","seed","output_parser"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19.post2"},"id":"OpenAIModel-0Us0w","description":"Generates text using OpenAI LLMs.","display_name":"OpenAI"},"selected":false,"width":384,"height":587},{"id":"MarkdownDataExtractorComponent-q71Rc","type":"genericNode","position":{"x":4567.080440191796,"y":1188.8116922647237},"data":{"type":"MarkdownDataExtractorComponent","node":{"template":{"_type":"Component","block_type":{"trace_as_metadata":true,"options":["csv","json","yaml"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"block_type","value":"csv","display_name":"Block Type (csv, json, yaml)","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema import Data\nimport csv\nfrom io import StringIO\nimport re\nimport json\nfrom typing import List, Dict, Any, Union\n\nclass MarkdownDataExtractorComponent(Component):\n    display_name = \"Extract Block from Markdown\"\n    description = \"Extracts data from markdown code blocks and converts it to a structured format.\"\n    documentation: str = \"https://docs.langflow.org/components/custom\"\n    icon = \"FileText\"\n    name = \"MarkdownDataExtractorComponent\"\n\n    inputs = [\n        MessageTextInput(name=\"input_value\", display_name=\"Markdown Text\", value=\"\"),\n        DropdownInput(name=\"block_type\", display_name=\"Block Type (csv, json, yaml)\", options=[\"csv\", \"json\", \"yaml\"], value=\"csv\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Extracted Data\", name=\"output\", method=\"build_output\"),\n    ]\n\n    def _extract_code_block(self, markdown_text: str, block_type: str) -> str:\n        \"\"\"Extract content from a specific type of code block in markdown text.\"\"\"\n        pattern = rf'```{block_type}\\n(.*?)\\n```'\n        match = re.search(pattern, markdown_text, re.DOTALL)\n        if not match:\n            raise ValueError(f\"No {block_type} content found between triple backticks.\")\n        return match.group(1)\n\n    def _parse_csv(self, csv_string: str) -> List[Dict[str, Any]]:\n        \"\"\"Parse CSV string into a list of dictionaries.\"\"\"\n        csv_file = StringIO(csv_string)\n        csv_reader = csv.DictReader(csv_file)\n        return [row for row in csv_reader]\n\n    def _parse_json(self, json_string: str) -> Union[Dict[str, Any], List[Dict[str, Any]]]:\n        \"\"\"Parse JSON string into a dictionary or list of dictionaries.\"\"\"\n        return json.loads(json_string)\n\n    def _parse_yaml(self, yaml_string: str) -> Union[Dict[str, Any], List[Dict[str, Any]]]:\n        \"\"\"Parse YAML string into a dictionary or list of dictionaries.\"\"\"\n        import yaml\n        return yaml.safe_load(yaml_string)\n\n    def build_output(self) -> Union[List[Data], Data]:\n        try:\n            block_type = self.block_type.lower()\n            \n            if block_type not in ['csv', 'json', 'yaml']:\n                raise ValueError(\"Invalid block_type. Must be 'csv', 'json', or 'yaml'.\")\n            \n            extracted_content = self._extract_code_block(self.input_value, block_type)\n            \n            if block_type == 'csv':\n                data = self._parse_csv(extracted_content)\n            elif block_type == 'json':\n                data = self._parse_json(extracted_content)\n            elif block_type == 'yaml':\n                data = self._parse_yaml(extracted_content)\n            \n            if isinstance(data, list):\n                data_objects = [Data(**entry) for entry in data]\n                self.status = data_objects\n                return data_objects\n            elif isinstance(data, dict):\n                self.status = Data(**data)\n                return self.status\n            else:\n                raise ValueError(f\"Unexpected data format from {block_type} parsing.\")\n\n        except ValueError as ve:\n            self.status = f\"Error: {str(ve)}\"\n            return Data(error=str(ve))\n        except json.JSONDecodeError as je:\n            self.status = f\"JSON Decode Error: {str(je)}\"\n            return Data(error=f\"JSON Decode Error: {str(je)}\")\n        except yaml.YAMLError as ye:\n            self.status = f\"YAML Parse Error: {str(ye)}\"\n            return Data(error=f\"YAML Parse Error: {str(ye)}\")\n        except Exception as e:\n            self.status = f\"Unexpected Error: {str(e)}\"\n            return Data(error=f\"Unexpected Error: {str(e)}\")","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Markdown Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Extracts data from markdown code blocks and converts it to a structured format.","icon":"FileText","base_classes":["Data"],"display_name":"Extract Block From Markdown","documentation":"https://docs.langflow.org/components/custom","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":true,"outputs":[{"types":["Data"],"selected":"Data","name":"output","display_name":"Extracted Data","method":"build_output","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","block_type"],"beta":false,"edited":true,"lf_version":"1.0.19.post2"},"id":"MarkdownDataExtractorComponent-q71Rc"},"selected":false,"width":384,"height":391,"dragging":false},{"id":"MultiSpeakerAudioGenerator-B3NAE","type":"genericNode","position":{"x":5643.573015300722,"y":1186.7631958564507},"data":{"type":"MultiSpeakerAudioGenerator","node":{"template":{"_type":"Component","input_value":{"trace_as_metadata":true,"list":false,"trace_as_input":true,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Data Input Dialogue","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The Input Data here should have the keys Speaker, Dialogue and Duration (in seconds). Here is an example: \nSpeaker, Dialogue, Duration\nPerson A, Some Dialogue Here , 20\nPerson B, Some Other Here , 10","title_case":false,"type":"other","_input_type":"DataInput"},"api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"name":"api_key","value":"","display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom langflow.custom import Component\nfrom langflow.io import Output, DataInput, SecretStrInput\nfrom openai import OpenAI\nimport os\nfrom langflow.schema import Data\nfrom pydub import AudioSegment\nimport tempfile\n\nclass MultiSpeakerAudioGenerator(Component):\n    display_name = \"OpenAI Whisper Data to Speech\"\n    description = \"Generates a single speech audio file from dialogue data with multiple speakers using OpenAI's TTS API.\"\n    documentation: str = \"https://platform.openai.com/docs/guides/text-to-speech\"\n    icon = \"OpenAI\"\n    name = \"MultiSpeakerAudioGenerator\"\n    inputs = [\n        DataInput(name=\"input_value\", display_name=\"Data Input Dialogue\", info=\"The Input Data here should have the keys Speaker, Dialogue and Duration (in seconds). Here is an example: \\nSpeaker, Dialogue, Duration\\nPerson A, Some Dialogue Here , 20\\nPerson B, Some Other Here , 10\"),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        MessageTextInput(\n            name=\"filepath\",\n            display_name=\"File Path to Save\",\n            info=\"File Path to save audio result (Do NOT include file extension), it will always be saved as mp3.\",\n            advanced=False,\n            value=\"audio_result\"\n        )\n    ]\n    outputs = [\n        Output(display_name=\"Generated Audio Output\", name=\"output\", method=\"build_output\"),\n    ]\n\n    def build_output(self) -> Data:\n        \"\"\"\n        Generates a single speech audio file from dialogue data with multiple speakers using OpenAI's TTS API.\n        \"\"\"\n        # Validate the API key\n        if not self.api_key:\n            raise ValueError(\"OpenAI API key must be provided.\")\n        \n        client = OpenAI(api_key=self.api_key)\n        \n        if not client.api_key:\n            raise ValueError(\"Invalid OpenAI API key set in environment variable 'OPENAI_API_KEY'.\")\n\n        # Initialize empty audio segment\n        combined_audio = AudioSegment.silent(duration=0)\n        output_info = []\n        available_voices = ['alloy', 'nova', 'echo', 'fable', 'onyx', 'shimmer']\n        speaker_voice_map = {}\n        voice_index = 0\n\n        # Create a temporary directory to store intermediate audio files\n        with tempfile.TemporaryDirectory() as temp_dir:\n            for idx, data_item in enumerate(self.input_value):\n                # Extract speaker and dialogue information\n                try:\n                    speaker = data_item.data['Speaker']\n                    dialogue = data_item.data['Dialogue']\n                except KeyError as e:\n                    raise ValueError(f\"Missing required data field: {str(e)}\")\n                \n                # Assign a voice to each speaker sequentially\n                if speaker not in speaker_voice_map:\n                    speaker_voice_map[speaker] = available_voices[voice_index % len(available_voices)]\n                    voice_index += 1\n                \n                voice = speaker_voice_map[speaker]\n\n                # Generate speech audio\n                try:\n                    response = client.audio.speech.create(\n                        model=\"tts-1\",\n                        voice=voice,\n                        input=dialogue\n                    )\n                except Exception as e:\n                    raise RuntimeError(f\"An error occurred while generating speech for {speaker}: {e}\")\n                \n                # Save the generated audio to a temporary file\n                temp_audio_file = os.path.join(temp_dir, f\"temp_{idx}.mp3\")\n                response.stream_to_file(temp_audio_file)\n\n                # Append the generated audio to the combined audio segment\n                try:\n                    audio_segment = AudioSegment.from_mp3(temp_audio_file)\n                    combined_audio += audio_segment\n                except Exception as e:\n                    raise RuntimeError(f\"An error occurred while processing the audio file for {speaker}: {e}\")\n                \n                # Add speaker dialogue info to output\n                output_info.append({\n                    'speaker': speaker,\n                    'dialogue': dialogue,\n                    'voice': voice\n                })\n\n        # Export the final combined audio file\n        final_audio_file = f\"{self.filepath}.mp3\"\n        try:\n            combined_audio.export(final_audio_file, format=\"mp3\")\n        except Exception as e:\n            raise RuntimeError(f\"Failed to export combined audio file: {e}\")\n\n        # Return the data output\n        return Data(\n            name=\"output\",\n            data={\n                'audio_file': final_audio_file,\n                'dialogue_info': output_info\n            },\n            text_key=\"audio_file\"\n        )","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"filepath":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"filepath","value":"audio_result_2m","display_name":"File Path to Save","advanced":false,"input_types":["Message"],"dynamic":false,"info":"File Path to save audio result (Do NOT include file extension), it will always be saved as mp3.","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Generates a single speech audio file from dialogue data with multiple speakers using OpenAI's TTS API.","icon":"OpenAI","base_classes":["Data"],"display_name":"OpenAI Whisper Data to Speech","documentation":"https://platform.openai.com/docs/guides/text-to-speech","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"output","display_name":"Generated Audio Output","method":"build_output","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","api_key","filepath"],"beta":false,"edited":true,"lf_version":"1.0.19.post2"},"id":"MultiSpeakerAudioGenerator-B3NAE"},"selected":false,"width":384,"height":455,"positionAbsolute":{"x":5643.573015300722,"y":1186.7631958564507},"dragging":false},{"id":"TextInput-zyk18","type":"genericNode","position":{"x":3244.699629105386,"y":179.8861370055654},"data":{"type":"TextInput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"1. Begin with a brief introduction and end with a conclusion.\n2. Maintain a natural conversation flow between Person A and Person B.\n3. Include pauses, interruptions, and overlapping speech where appropriate.\n4. Vary the length of each dialogue segment (5-30 seconds) for a more dynamic conversation.\n5. Incorporate relevant sound effects or background noises to enhance the audio experience.\n6. Use a diverse range of tones and emotions to bring the conversation to life.\n7. Ensure the content is engaging, informative, and tailored to the topic.\n8. Include timestamps for each dialogue segment, ensuring the total duration matches.","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as input.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Get text inputs from the Playground.","icon":"type","base_classes":["Message"],"display_name":"Instructions","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":true,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":false},"id":"TextInput-zyk18","showNode":false},"selected":false,"width":96,"height":96,"dragging":false,"positionAbsolute":{"x":3244.699629105386,"y":179.8861370055654}},{"id":"TextInput-pJsVV","type":"genericNode","position":{"x":1456.1771593638423,"y":624.5247709358321},"data":{"type":"TextInput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"```csv\nSpeaker,Dialogue,Duration\nPerson A,\"Hello and welcome to our podcast!\"\nPerson B,\"Thanks for having me. I'm excited to discuss [topic] today.\"\n...","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as input.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Get text inputs from the Playground.","icon":"type","base_classes":["Message"],"display_name":"Output Format","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":true,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":false},"id":"TextInput-pJsVV","showNode":false},"selected":false,"width":96,"height":96,"dragging":false,"positionAbsolute":{"x":1456.1771593638423,"y":624.5247709358321}},{"id":"TextInput-5fqFk","type":"genericNode","position":{"x":3240.4587001009495,"y":80.17602905322701},"data":{"type":"TextInput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"- Be specific in describing tones (e.g., sarcastic, curious, professional) and emotions (e.g., surprised, amused, concerned).\n- Include non-verbal cues in the dialogue when relevant (e.g., \"[laughs]\", \"[sighs]\").\n- For sound effects, be precise (e.g., \"Phone ringing\", \"Door creaking\", \"Applause\").\n- Ensure the content is coherent, factual, and aligns with the provided data.\n- If the data includes technical terms or complex concepts, incorporate brief explanations or analogies to make the content accessible to a general audience.\n\nPlease generate a detailed, engaging, and realistic podcast script based on these guidelines.","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as input.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Get text inputs from the Playground.","icon":"type","base_classes":["Message"],"display_name":"Additional Details","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":true,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":false},"id":"TextInput-5fqFk","showNode":false},"selected":false,"width":96,"height":96,"dragging":false},{"id":"TextInput-afqWT","type":"genericNode","position":{"x":3246.9638854042587,"y":278.29303352612703},"data":{"type":"TextInput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"Two","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as input.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Get text inputs from the Playground.","icon":"type","base_classes":["Message"],"display_name":"Number of Speakers","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":false},"id":"TextInput-afqWT","showNode":false},"selected":false,"width":96,"height":96,"dragging":false},{"id":"skeleton-of-thought-fwOVj","type":"genericNode","position":{"x":2300.778220613876,"y":768.750414237403},"data":{"type":"skeleton-of-thought","node":{"template":{"_type":"Component","llm":{"trace_as_metadata":true,"list":false,"required":true,"placeholder":"","show":true,"name":"llm","value":"","display_name":"Language Model","advanced":false,"input_types":["LanguageModel"],"dynamic":false,"info":"An LLM Executable Object.","title_case":false,"type":"other","_input_type":"HandleInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"# from langflow.field_typing import Data\nfrom langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema import Data\n\n\nimport importlib\nimport subprocess\nimport sys\nfrom typing import Dict, Any, List\nfrom pydantic import BaseModel\nimport logging\nimport json\n\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate, ChatPromptTemplate\nfrom langflow.base.agents.agent import LCToolsAgentComponent\nfrom langflow.inputs.inputs import HandleInput, IntInput\nfrom langflow.schema.message import Message\n\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\nfrom langflow.io import MessageTextInput, Output\n\nclass SkeletonChain(Component):\n    display_name = \"Skeleton of Thoughts Chain\"\n    description = \"A component that implements Skeleton Thought Chain (Chain Planning)\"\n    documentation: str = \"https://python.langchain.com/v0.1/docs/templates/skeleton-of-thought/\"\n    icon = \"LangChain\"\n    name = \"skeleton-of-thought\"\n\n    inputs = [\n        HandleInput(\n            name=\"llm\",\n            display_name=\"Language Model\",\n            input_types=[\"LanguageModel\"],\n            required=True,\n            info=\"An LLM Executable Object.\"\n        ),\n        MessageTextInput(\n            name=\"input\",\n            display_name=\"Input\",\n            value=\"Hello, World!\",\n            info=\"The question or topic you want to explore using the Skeleton of Thoughts method.\"\n        ),\n        IntInput(\n            name=\"number_of_topics\",\n            display_name=\"Number of Topics\",\n            value=5,\n            info=\"The number of main points in the skeleton. It works better choicing a range between 1 and 20.\"\n        ),\n    ]\n    \n    outputs = [\n        Output(display_name=\"Output\",name=\"output\",method=\"build_thoughts_data\"),\n    ]\n    \n    class ChainInput(BaseModel):\n        question: str\n\n    @staticmethod\n    def parse_numbered_list(input_str: str) -> List[Dict[str, Any]]:\n        \"\"\"\n        Parses a numbered list into a list of dictionaries.\n\n        Each element having two keys:\n        'point_index' for the index in the numbered list, and 'point_skeleton' for the content.\n        \"\"\"\n        lines = input_str.strip().split(\"\\n\")\n        parsed_list = []\n    \n        for line in lines:\n            parts = line.split(\". \", 1)\n            if len(parts) == 2:\n                try:\n                    index = int(parts[0])\n                    point = parts[1].strip()\n                    parsed_list.append({\"point_index\": index, \"point_skeleton\": point})\n                except ValueError:\n                    continue  # Skip lines that don't start with a valid number\n    \n        return parsed_list\n\n    @staticmethod\n    def create_list_elements(_input: Dict[str, Any]) -> List[Dict[str, Any]]:\n        \"\"\"\n        Creates a list of elements from the input skeleton and question.\n        \"\"\"\n        skeleton = _input[\"skeleton\"]\n        numbered_list = SkeletonChain.parse_numbered_list(skeleton)\n        return [{\"skeleton\": skeleton, \"question\": _input[\"question\"], **el} for el in numbered_list]\n\n    @staticmethod\n    def get_final_answer(expanded_list: List[str]) -> Dict[str, Dict[str, str]]:\n        \"\"\"\n        Constructs a dictionary of answers from an expanded list.\n        Each entry includes a title and description.\n        \"\"\"\n        return {\n            f\"point_{i+1}\": {\n                \"point_title\": \" \".join(el.split()[:3]),\n                \"point_description\": el\n            }\n            for i, el in enumerate(expanded_list)\n        }\n    \n\n    def build_thoughts_data(self) -> Data:\n        \"\"\"\n        Builds the output for the component.\n        \"\"\"\n        logger.info(\"Starting build_output...\")\n        if self.number_of_topics <= 0:\n            raise ValueError(\"The Number of Points should be a Positive Value!\")\n\n        # Refined Prompt Template to enforce JSON format\n        skeleton_generator_template = f\"\"\"You're an organizer responsible for only \\\n        giving the skeleton (not the full content) for answering the question.\n        Provide the skeleton as a numbered list of points to answer \\\n        the question. Each skeleton point should be very short \\\n        with only 3–5 words. \\\n        The skeleton must have {self.number_of_topics} points. Now, please provide the skeleton \\\n        for the following question.\n        {{question}}\n        Skeleton:\"\"\"\n\n        skeleton_generator_prompt = ChatPromptTemplate.from_template(\n            skeleton_generator_template\n        )\n        \n        logger.debug(\"Skeleton Generator Prompt Created.\")\n\n        skeleton_generator_chain = (\n            skeleton_generator_prompt | self.llm | StrOutputParser()\n        )\n\n        logger.debug(\"Skeleton Generator Chain Created.\")\n\n        # Updated Point Expander Template for clarity\n        point_expander_template = \"\"\"You're responsible for continuing \\\n        the writing of one and only one point in the overall answer to the following question.\n        Question: {question}\n        The skeleton of the answer is:\n        {skeleton}\n        Continue and only continue the writing of point {point_index}. \\\n        Write it **very shortly** in 1–2 sentences and do not continue with other points!\n        Point {point_index}. {point_skeleton}\"\"\"\n\n        point_expander_prompt = ChatPromptTemplate.from_template(point_expander_template)\n        \n        logger.debug(\"Point Expander Prompt Created.\")\n\n        point_expander_chain = RunnablePassthrough.assign(\n            continuation=point_expander_prompt | self.llm | StrOutputParser()\n        ) | (lambda x: x[\"point_skeleton\"].strip() + \" \" + x[\"continuation\"])\n        \n        logger.debug(\"Point Expander Chain Created.\")\n\n        # Define the entire chain\n        chain = (\n            RunnablePassthrough.assign(skeleton=skeleton_generator_chain)\n            | self.create_list_elements\n            | point_expander_chain.map()\n            | self.get_final_answer\n        ).with_types(input_type=self.ChainInput)\n        \n        logger.debug(\"Chain Defined.\")\n\n        # Invoke the chain with the input question\n        chain_output = chain.invoke({\"question\": self.input})\n        \n        logger.debug(f\"Chain Output: {chain_output}\")\n\n        # Extract the final answer\n        try:\n            model_output = json.dumps(chain_output, indent=2)\n            logger.debug(f\"Model Output (JSON): {model_output}\")\n        except (TypeError, ValueError) as e:\n            logger.error(f\"Error converting chain_output to JSON string: {e}\")\n            model_output = str(chain_output)  # Fallback to string representation\n\n        # Ensure that page_content is a string\n        if not isinstance(model_output, str):\n            logger.error(\"model_output is not a string. Converting to string.\")\n            model_output = str(model_output)\n        \n        converted_output = [{\"result\": i} for i in range(1, len(model_output) + 1)]\n        \n        parsed_output = json.loads(model_output)\n    \n        # Convert to the desired format\n        converted_output = [{\n        f\"point_{i}\": parsed_output[f\"point_{i}\"][\"point_description\"]\n        for i in range(1, len(parsed_output) + 1)\n        }]\n        \n        return Data(data=converted_output[0])","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The question or topic you want to explore using the Skeleton of Thoughts method.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"number_of_topics":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"number_of_topics","value":3,"display_name":"Number of Topics","advanced":false,"dynamic":false,"info":"The number of main points in the skeleton. It works better choicing a range between 1 and 20.","title_case":false,"type":"int","_input_type":"IntInput","load_from_db":false}},"description":"A component that implements Skeleton Thought Chain (Chain Planning)","icon":"LangChain","base_classes":["Data"],"display_name":"Skeleton of Thoughts Chain","documentation":"https://python.langchain.com/v0.1/docs/templates/skeleton-of-thought/","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"output","display_name":"Output","method":"build_thoughts_data","value":"__UNDEFINED__","cache":true}],"field_order":["llm","input","number_of_topics"],"beta":false,"edited":true,"lf_version":"1.0.19.post2"},"id":"skeleton-of-thought-fwOVj"},"selected":false,"width":384,"height":439,"dragging":false,"positionAbsolute":{"x":2300.778220613876,"y":768.750414237403}},{"id":"ParseData-nK4P9","type":"genericNode","position":{"x":2889.0413620981735,"y":939.2683581681479},"data":{"type":"ParseData","node":{"template":{"_type":"Component","data":{"trace_as_metadata":true,"list":false,"trace_as_input":true,"required":false,"placeholder":"","show":true,"name":"data","value":"","display_name":"Data","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The data to convert to text.","title_case":false,"type":"other","_input_type":"DataInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import DataInput, MultilineInput, Output, StrInput\nfrom langflow.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n    name = \"ParseData\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. \"\n            \"It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"sep":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"sep","value":"\n","display_name":"Separator","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"template":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"{point_2}","display_name":"Template","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Convert Data into plain text following a specified template.","icon":"braces","base_classes":["Message"],"display_name":"Parse Data","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"parse_data","value":"__UNDEFINED__","cache":true}],"field_order":["data","template","sep"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19.post2"},"id":"ParseData-nK4P9","description":"Convert Data into plain text following a specified template.","display_name":"Parse Data"},"selected":false,"width":384,"height":353},{"id":"ParseData-U5b53","type":"genericNode","position":{"x":2876.989330234971,"y":490.36756313224134},"data":{"type":"ParseData","node":{"template":{"_type":"Component","data":{"trace_as_metadata":true,"list":false,"trace_as_input":true,"required":false,"placeholder":"","show":true,"name":"data","value":"","display_name":"Data","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The data to convert to text.","title_case":false,"type":"other","_input_type":"DataInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import DataInput, MultilineInput, Output, StrInput\nfrom langflow.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n    name = \"ParseData\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. \"\n            \"It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"sep":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"sep","value":"\n","display_name":"Separator","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"template":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"{point_1}","display_name":"Template","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Convert Data into plain text following a specified template.","icon":"braces","base_classes":["Message"],"display_name":"Parse Data","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"parse_data","value":"__UNDEFINED__","cache":true}],"field_order":["data","template","sep"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19.post2"},"id":"ParseData-U5b53","description":"Convert Data into plain text following a specified template.","display_name":"Parse Data"},"selected":false,"width":384,"height":353},{"id":"Prompt-cJv4n","type":"genericNode","position":{"x":3543.3945399691506,"y":106.45558526624512},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"# LLM Prompt: Generate Detailed Podcast Script\n\nUsing the provided data: {data}, create a comprehensive podcast script featuring {number_of_speakers} speakers, Person A, Person B, ..., with a total duration of {PodcastDuration}. The output should be in CSV format with the following columns: Speaker, Dialogue and Duration (in seconds).\n\n## Instructions:\n\nYou are responsible to create the first part (OPENING) of the Podcast (with opening) with the theme of {theme}.\nTry creating this first part with 5 minutes.\n\n## Output Format:\n\n{output_format}\n\n## Additional Notes:\n\n{additional_details}","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput"},"PodcastDuration":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"PodcastDuration","display_name":"PodcastDuration","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"data":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"data","display_name":"data","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"output_format":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"output_format","display_name":"output_format","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"additional_details":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"additional_details","display_name":"additional_details","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"number_of_speakers":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"number_of_speakers","display_name":"number_of_speakers","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"theme":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"theme","display_name":"theme","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["data","number_of_speakers","PodcastDuration","theme","output_format","additional_details"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false,"lf_version":"1.0.19.post2"},"id":"Prompt-cJv4n"},"selected":false,"width":384,"height":821},{"id":"Prompt-DJXYx","type":"genericNode","position":{"x":3555.8442575086847,"y":1008.4550378127285},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"# LLM Prompt: Generate Detailed Podcast Script\n\nUsing the provided data: {data}, create a comprehensive podcast script featuring {number_of_speakers} speakers, Person A, Person B, ..., with a total duration of {PodcastDuration}. The output should be in CSV format with the following columns: Speaker, Dialogue and Duration (in seconds).\n\n## Instructions:\n\nYou are responsible to create the MIDDLE Part of the Podcast you must talk about: {theme}.\nTry creating this part with at least 5 minutes.\n\n## Output Format:\n\n{output_format}\n\n## Additional Notes:\n\n{additional_details}","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput"},"PodcastDuration":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"PodcastDuration","display_name":"PodcastDuration","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"data":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"data","display_name":"data","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"output_format":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"output_format","display_name":"output_format","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"additional_details":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"additional_details","display_name":"additional_details","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"number_of_speakers":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"number_of_speakers","display_name":"number_of_speakers","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"theme":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"theme","display_name":"theme","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["data","number_of_speakers","PodcastDuration","theme","output_format","additional_details"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":true,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false,"lf_version":"1.0.19.post2"},"id":"Prompt-DJXYx"},"selected":false,"width":384,"height":821},{"id":"TextOutput-TXF0h","type":"genericNode","position":{"x":4442.350536332622,"y":615.8786556411943},"data":{"type":"TextOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextOutputComponent(TextComponent):\n    display_name = \"Text Output\"\n    description = \"Display a text output in the Playground.\"\n    icon = \"type\"\n    name = \"TextOutput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as output.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        self.status = self.input_value\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as output.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Display a text output in the Playground.","icon":"type","base_classes":["Message"],"display_name":"Podcast - Parte 1","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":false},"id":"TextOutput-TXF0h","showNode":false},"selected":false,"width":96,"height":96,"dragging":false},{"id":"OpenAIModel-lJEAI","type":"genericNode","position":{"x":3989.1630376782423,"y":1102.1490723928687},"data":{"type":"OpenAIModel","node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"name":"api_key","value":"","display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    SecretStrInput,\n    StrInput,\n)\nfrom langflow.inputs.inputs import HandleInput\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. \"\n            \"You must pass the word JSON in the prompt. \"\n            \"If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. \"\n            \"Defaults to https://api.openai.com/v1. \"\n            \"You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        api_key = SecretStr(openai_api_key) if openai_api_key else None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})\n\n        return output\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"json_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"json_mode","value":false,"display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool","_input_type":"BoolInput"},"max_tokens":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"model_kwargs","value":{},"display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"dict","_input_type":"DictInput"},"model_name":{"trace_as_metadata":true,"options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"gpt-4o","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput","load_from_db":false},"openai_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"openai_api_base","value":"","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput"},"output_schema":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"name":"output_schema","value":{},"display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.","title_case":false,"type":"dict","_input_type":"DictInput"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"seed","value":1,"display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0.1,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generates text using OpenAI LLMs.","icon":"OpenAI","base_classes":["LanguageModel","Message"],"display_name":"OpenAI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":["input_value","stream","system_message"]},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":["api_key","json_mode","max_tokens","model_kwargs","model_name","openai_api_base","output_schema","seed","temperature"]}],"field_order":["input_value","system_message","stream","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","api_key","temperature","seed","output_parser"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19.post2"},"id":"OpenAIModel-lJEAI","description":"Generates text using OpenAI LLMs.","display_name":"OpenAI"},"selected":false,"width":384,"height":587},{"id":"TextOutput-qXA05","type":"genericNode","position":{"x":4422.477796512934,"y":1386.302238439698},"data":{"type":"TextOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextOutputComponent(TextComponent):\n    display_name = \"Text Output\"\n    description = \"Display a text output in the Playground.\"\n    icon = \"type\"\n    name = \"TextOutput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as output.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        self.status = self.input_value\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as output.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Display a text output in the Playground.","icon":"type","base_classes":["Message"],"display_name":"Podcast - Parte 2","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":true,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":false,"lf_version":"1.0.18"},"id":"TextOutput-qXA05","showNode":false},"selected":false,"width":96,"height":96,"dragging":false},{"id":"ParseData-5dSHz","type":"genericNode","position":{"x":2897.447854238395,"y":1397.8557837324038},"data":{"type":"ParseData","node":{"template":{"_type":"Component","data":{"trace_as_metadata":true,"list":false,"trace_as_input":true,"required":false,"placeholder":"","show":true,"name":"data","value":"","display_name":"Data","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The data to convert to text.","title_case":false,"type":"other","_input_type":"DataInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import DataInput, MultilineInput, Output, StrInput\nfrom langflow.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n    name = \"ParseData\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. \"\n            \"It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"sep":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"sep","value":"\n","display_name":"Separator","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"template":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"{point_3}","display_name":"Template","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Convert Data into plain text following a specified template.","icon":"braces","base_classes":["Message"],"display_name":"Parse Data","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"parse_data","value":"__UNDEFINED__","cache":true}],"field_order":["data","template","sep"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19.post2"},"id":"ParseData-5dSHz","description":"Convert Data into plain text following a specified template.","display_name":"Parse Data"},"selected":false,"width":384,"height":353,"dragging":false},{"id":"Prompt-vMF3H","type":"genericNode","position":{"x":3549.8872791529434,"y":1900.2547085179078},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"# LLM Prompt: Generate Detailed Podcast Script\n\nUsing the provided data: {data}, create a comprehensive podcast script featuring {number_of_speakers} speakers, Person A, Person B, ..., with a total duration of {PodcastDuration}. The output should be in CSV format with the following columns: Speaker, Dialogue and Duration (in seconds).\n\n## Instructions:\n\nYou are responsible to create the MIDDLE Part of the Podcast you must talk about: {theme}.\nTry creating this part with at least 5 minutes.\n\n## Output Format:\n\n{output_format}\n\n## Additional Notes:\n\n{additional_details}","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput"},"PodcastDuration":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"PodcastDuration","display_name":"PodcastDuration","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"data":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"data","display_name":"data","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"output_format":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"output_format","display_name":"output_format","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"additional_details":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"additional_details","display_name":"additional_details","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"number_of_speakers":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"number_of_speakers","display_name":"number_of_speakers","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"theme":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"theme","display_name":"theme","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["data","number_of_speakers","PodcastDuration","theme","output_format","additional_details"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false,"lf_version":"1.0.19.post2"},"id":"Prompt-vMF3H"},"selected":false,"width":384,"height":821},{"id":"OpenAIModel-WUq0W","type":"genericNode","position":{"x":4035.519978254219,"y":2037.9568368406651},"data":{"type":"OpenAIModel","node":{"template":{"_type":"Component","api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"name":"api_key","value":"","display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    SecretStrInput,\n    StrInput,\n)\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = LCModelComponent._base_inputs + [\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        if openai_api_key:\n            api_key = SecretStr(openai_api_key)\n        else:\n            api_key = None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")  # type: ignore\n            if message:\n                return message\n        return\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"json_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"json_mode","value":false,"display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool","_input_type":"BoolInput"},"max_tokens":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"model_kwargs","value":{},"display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"dict","_input_type":"DictInput"},"model_name":{"trace_as_metadata":true,"options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"gpt-4o","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput","load_from_db":false},"openai_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"openai_api_base","value":"","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput"},"output_schema":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"name":"output_schema","value":{},"display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.","title_case":false,"type":"dict","_input_type":"DictInput"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"seed","value":1,"display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0.1,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generates text using OpenAI LLMs.","icon":"OpenAI","base_classes":["LanguageModel","Message"],"display_name":"OpenAI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","system_message","stream","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","api_key","temperature","seed"],"beta":false,"edited":false,"lf_version":"1.0.19.post2"},"id":"OpenAIModel-WUq0W"},"selected":false,"width":384,"height":587},{"id":"TextOutput-R5U9G","type":"genericNode","position":{"x":4491.005905015507,"y":2349.5603807255907},"data":{"type":"TextOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextOutputComponent(TextComponent):\n    display_name = \"Text Output\"\n    description = \"Display a text output in the Playground.\"\n    icon = \"type\"\n    name = \"TextOutput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as output.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        self.status = self.input_value\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as output.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Display a text output in the Playground.","icon":"type","base_classes":["Message"],"display_name":"Podcast - Parte 3","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":false},"id":"TextOutput-R5U9G","showNode":false},"selected":false,"width":96,"height":96},{"id":"MarkdownDataExtractorComponent-T3UnQ","type":"genericNode","position":{"x":4610.4386323180715,"y":2179.9920713213205},"data":{"type":"MarkdownDataExtractorComponent","node":{"template":{"_type":"Component","block_type":{"trace_as_metadata":true,"options":["csv","json","yaml"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"block_type","value":"csv","display_name":"Block Type (csv, json, yaml)","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema import Data\nimport csv\nfrom io import StringIO\nimport re\nimport json\nfrom typing import List, Dict, Any, Union\n\nclass MarkdownDataExtractorComponent(Component):\n    display_name = \"Extract Block from Markdown\"\n    description = \"Extracts data from markdown code blocks and converts it to a structured format.\"\n    documentation: str = \"https://docs.langflow.org/components/custom\"\n    icon = \"FileText\"\n    name = \"MarkdownDataExtractorComponent\"\n\n    inputs = [\n        MessageTextInput(name=\"input_value\", display_name=\"Markdown Text\", value=\"\"),\n        DropdownInput(name=\"block_type\", display_name=\"Block Type (csv, json, yaml)\", options=[\"csv\", \"json\", \"yaml\"], value=\"csv\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Extracted Data\", name=\"output\", method=\"build_output\"),\n    ]\n\n    def _extract_code_block(self, markdown_text: str, block_type: str) -> str:\n        \"\"\"Extract content from a specific type of code block in markdown text.\"\"\"\n        pattern = rf'```{block_type}\\n(.*?)\\n```'\n        match = re.search(pattern, markdown_text, re.DOTALL)\n        if not match:\n            raise ValueError(f\"No {block_type} content found between triple backticks.\")\n        return match.group(1)\n\n    def _parse_csv(self, csv_string: str) -> List[Dict[str, Any]]:\n        \"\"\"Parse CSV string into a list of dictionaries.\"\"\"\n        csv_file = StringIO(csv_string)\n        csv_reader = csv.DictReader(csv_file)\n        return [row for row in csv_reader]\n\n    def _parse_json(self, json_string: str) -> Union[Dict[str, Any], List[Dict[str, Any]]]:\n        \"\"\"Parse JSON string into a dictionary or list of dictionaries.\"\"\"\n        return json.loads(json_string)\n\n    def _parse_yaml(self, yaml_string: str) -> Union[Dict[str, Any], List[Dict[str, Any]]]:\n        \"\"\"Parse YAML string into a dictionary or list of dictionaries.\"\"\"\n        import yaml\n        return yaml.safe_load(yaml_string)\n\n    def build_output(self) -> Union[List[Data], Data]:\n        try:\n            block_type = self.block_type.lower()\n            \n            if block_type not in ['csv', 'json', 'yaml']:\n                raise ValueError(\"Invalid block_type. Must be 'csv', 'json', or 'yaml'.\")\n            \n            extracted_content = self._extract_code_block(self.input_value, block_type)\n            \n            if block_type == 'csv':\n                data = self._parse_csv(extracted_content)\n            elif block_type == 'json':\n                data = self._parse_json(extracted_content)\n            elif block_type == 'yaml':\n                data = self._parse_yaml(extracted_content)\n            \n            if isinstance(data, list):\n                data_objects = [Data(**entry) for entry in data]\n                self.status = data_objects\n                return data_objects\n            elif isinstance(data, dict):\n                self.status = Data(**data)\n                return self.status\n            else:\n                raise ValueError(f\"Unexpected data format from {block_type} parsing.\")\n\n        except ValueError as ve:\n            self.status = f\"Error: {str(ve)}\"\n            return Data(error=str(ve))\n        except json.JSONDecodeError as je:\n            self.status = f\"JSON Decode Error: {str(je)}\"\n            return Data(error=f\"JSON Decode Error: {str(je)}\")\n        except yaml.YAMLError as ye:\n            self.status = f\"YAML Parse Error: {str(ye)}\"\n            return Data(error=f\"YAML Parse Error: {str(ye)}\")\n        except Exception as e:\n            self.status = f\"Unexpected Error: {str(e)}\"\n            return Data(error=f\"Unexpected Error: {str(e)}\")","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Markdown Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Extracts data from markdown code blocks and converts it to a structured format.","icon":"FileText","base_classes":["Data"],"display_name":"Extract Block From Markdown","documentation":"https://docs.langflow.org/components/custom","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"output","display_name":"Extracted Data","method":"build_output","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","block_type"],"beta":false,"edited":true,"lf_version":"1.0.19.post2"},"id":"MarkdownDataExtractorComponent-T3UnQ"},"selected":false,"width":384,"height":391},{"id":"OpenAIModel-ZQ7eE","type":"genericNode","position":{"x":1797.4211518148027,"y":53.921875},"data":{"type":"OpenAIModel","node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"name":"api_key","value":"","display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    SecretStrInput,\n    StrInput,\n)\nfrom langflow.inputs.inputs import HandleInput\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. \"\n            \"You must pass the word JSON in the prompt. \"\n            \"If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. \"\n            \"Defaults to https://api.openai.com/v1. \"\n            \"You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        api_key = SecretStr(openai_api_key) if openai_api_key else None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})\n\n        return output\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"json_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"json_mode","value":false,"display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool","_input_type":"BoolInput"},"max_tokens":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"model_kwargs","value":{},"display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"dict","_input_type":"DictInput"},"model_name":{"trace_as_metadata":true,"options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"gpt-4o-mini","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput"},"openai_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"openai_api_base","value":"","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput"},"output_schema":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"name":"output_schema","value":{},"display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.","title_case":false,"type":"dict","_input_type":"DictInput"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"seed","value":1,"display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0.1,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generates text using OpenAI LLMs.","icon":"OpenAI","base_classes":["LanguageModel","Message"],"display_name":"OpenAI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":["input_value","stream","system_message"]},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":["api_key","json_mode","max_tokens","model_kwargs","model_name","openai_api_base","output_schema","seed","temperature"]}],"field_order":["input_value","system_message","stream","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","api_key","temperature","seed","output_parser"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19.post2"},"id":"OpenAIModel-ZQ7eE","description":"Generates text using OpenAI LLMs.","display_name":"OpenAI"},"selected":false,"width":384,"height":587},{"id":"MarkdownDataExtractorComponent-4JVPI","type":"genericNode","position":{"x":4598.329712638297,"y":472.280668304688},"data":{"type":"MarkdownDataExtractorComponent","node":{"template":{"_type":"Component","block_type":{"trace_as_metadata":true,"options":["csv","json","yaml"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"block_type","value":"csv","display_name":"Block Type (csv, json, yaml)","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema import Data\nimport csv\nfrom io import StringIO\nimport re\nimport json\nfrom typing import List, Dict, Any, Union\n\nclass MarkdownDataExtractorComponent(Component):\n    display_name = \"Extract Block from Markdown\"\n    description = \"Extracts data from markdown code blocks and converts it to a structured format.\"\n    documentation: str = \"https://docs.langflow.org/components/custom\"\n    icon = \"FileText\"\n    name = \"MarkdownDataExtractorComponent\"\n\n    inputs = [\n        MessageTextInput(name=\"input_value\", display_name=\"Markdown Text\", value=\"\"),\n        DropdownInput(name=\"block_type\", display_name=\"Block Type (csv, json, yaml)\", options=[\"csv\", \"json\", \"yaml\"], value=\"csv\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Extracted Data\", name=\"output\", method=\"build_output\"),\n    ]\n\n    def _extract_code_block(self, markdown_text: str, block_type: str) -> str:\n        \"\"\"Extract content from a specific type of code block in markdown text.\"\"\"\n        pattern = rf'```{block_type}\\n(.*?)\\n```'\n        match = re.search(pattern, markdown_text, re.DOTALL)\n        if not match:\n            raise ValueError(f\"No {block_type} content found between triple backticks.\")\n        return match.group(1)\n\n    def _parse_csv(self, csv_string: str) -> List[Dict[str, Any]]:\n        \"\"\"Parse CSV string into a list of dictionaries.\"\"\"\n        csv_file = StringIO(csv_string)\n        csv_reader = csv.DictReader(csv_file)\n        return [row for row in csv_reader]\n\n    def _parse_json(self, json_string: str) -> Union[Dict[str, Any], List[Dict[str, Any]]]:\n        \"\"\"Parse JSON string into a dictionary or list of dictionaries.\"\"\"\n        return json.loads(json_string)\n\n    def _parse_yaml(self, yaml_string: str) -> Union[Dict[str, Any], List[Dict[str, Any]]]:\n        \"\"\"Parse YAML string into a dictionary or list of dictionaries.\"\"\"\n        import yaml\n        return yaml.safe_load(yaml_string)\n\n    def build_output(self) -> Union[List[Data], Data]:\n        try:\n            block_type = self.block_type.lower()\n            \n            if block_type not in ['csv', 'json', 'yaml']:\n                raise ValueError(\"Invalid block_type. Must be 'csv', 'json', or 'yaml'.\")\n            \n            extracted_content = self._extract_code_block(self.input_value, block_type)\n            \n            if block_type == 'csv':\n                data = self._parse_csv(extracted_content)\n            elif block_type == 'json':\n                data = self._parse_json(extracted_content)\n            elif block_type == 'yaml':\n                data = self._parse_yaml(extracted_content)\n            \n            if isinstance(data, list):\n                data_objects = [Data(**entry) for entry in data]\n                self.status = data_objects\n                return data_objects\n            elif isinstance(data, dict):\n                self.status = Data(**data)\n                return self.status\n            else:\n                raise ValueError(f\"Unexpected data format from {block_type} parsing.\")\n\n        except ValueError as ve:\n            self.status = f\"Error: {str(ve)}\"\n            return Data(error=str(ve))\n        except json.JSONDecodeError as je:\n            self.status = f\"JSON Decode Error: {str(je)}\"\n            return Data(error=f\"JSON Decode Error: {str(je)}\")\n        except yaml.YAMLError as ye:\n            self.status = f\"YAML Parse Error: {str(ye)}\"\n            return Data(error=f\"YAML Parse Error: {str(ye)}\")\n        except Exception as e:\n            self.status = f\"Unexpected Error: {str(e)}\"\n            return Data(error=f\"Unexpected Error: {str(e)}\")","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Markdown Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Extracts data from markdown code blocks and converts it to a structured format.","icon":"FileText","base_classes":["Data"],"display_name":"Extract Block From Markdown","documentation":"https://docs.langflow.org/components/custom","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"output","display_name":"Extracted Data","method":"build_output","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","block_type"],"beta":false,"edited":true,"lf_version":"1.0.19.post2"},"id":"MarkdownDataExtractorComponent-4JVPI"},"selected":false,"width":384,"height":391},{"id":"MergeDataComponent-evw4H","type":"genericNode","position":{"x":5116.201714348517,"y":1226.8156703878758},"data":{"type":"MergeDataComponent","node":{"template":{"_type":"Component","data_inputs":{"trace_as_metadata":true,"list":true,"trace_as_input":true,"required":false,"placeholder":"","show":true,"name":"data_inputs","value":"","display_name":"Data Inputs","advanced":false,"input_types":["Data"],"dynamic":false,"info":"List of Data objects to be merged.","title_case":false,"type":"other","_input_type":"DataInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import List, Any\nfrom loguru import logger\nfrom langflow.custom import Component\nfrom langflow.io import DataInput, Output\nfrom langflow.schema import Data\n\nclass MergeDataComponent(Component):\n    display_name = \"Merge Data\"\n    description = \"Combines multiple Data objects into a single unified Data object, preserving lists and handling duplicates.\"\n    icon = \"merge\"\n    beta = False\n\n    inputs = [\n        DataInput(\n            name=\"data_inputs\",\n            display_name=\"Data Inputs\",\n            info=\"List of Data objects to be merged.\",\n            is_list=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Merged Data\", name=\"merged_data\", method=\"merge_data\"),\n    ]\n\n    def _merge_data(self, target: dict, source: Any, key: str) -> None:\n        if key not in target:\n            target[key] = source\n        else:\n            if isinstance(target[key], list):\n                if isinstance(source, list):\n                    target[key].extend(source)\n                else:\n                    target[key].append(source)\n            elif isinstance(target[key], dict) and isinstance(source, dict):\n                for k, v in source.items():\n                    self._merge_data(target[key], v, k)\n            else:\n                target[key] = [target[key], source]\n\n    def merge_data(self) -> Data:\n        logger.info(\"Starting data merge process\")\n        data_inputs: List[Data] = self.data_inputs\n\n        if not data_inputs:\n            logger.warning(\"No data inputs provided\")\n            return Data()\n\n        if len(data_inputs) == 1:\n            logger.info(\"Only one data input provided, returning as is\")\n            return data_inputs[0]\n\n        logger.info(\"Data merge completed successfully\")\n        return self.data_inputs","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false}},"description":"Combines multiple Data objects into a single unified Data object, preserving lists and handling duplicates.","icon":"merge","base_classes":["Data"],"display_name":"Merge Data","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"merged_data","display_name":"Merged Data","method":"merge_data","value":"__UNDEFINED__","cache":true}],"field_order":["data_inputs"],"beta":false,"edited":true,"lf_version":"1.0.19.post2"},"id":"MergeDataComponent-evw4H"},"selected":false,"width":384,"height":283}],"edges":[{"source":"File-55Dzi","target":"SplitText-DsAr8","sourceHandle":"{œdataTypeœ:œFileœ,œidœ:œFile-55Dziœ,œnameœ:œdataœ,œoutput_typesœ:[œDataœ]}","targetHandle":"{œfieldNameœ:œdata_inputsœ,œidœ:œSplitText-DsAr8œ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","id":"reactflow__edge-File-55Dzi{œdataTypeœ:œFileœ,œidœ:œFile-55Dziœ,œnameœ:œdataœ,œoutput_typesœ:[œDataœ]}-SplitText-DsAr8{œfieldNameœ:œdata_inputsœ,œidœ:œSplitText-DsAr8œ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"data_inputs","id":"SplitText-DsAr8","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"File","id":"File-55Dzi","name":"data","output_types":["Data"]}},"selected":false,"className":"","animated":false},{"source":"MergeDataComponent-wrxRM","target":"ParseData-2Umhs","sourceHandle":"{œdataTypeœ:œMergeDataComponentœ,œidœ:œMergeDataComponent-wrxRMœ,œnameœ:œmerged_dataœ,œoutput_typesœ:[œDataœ]}","targetHandle":"{œfieldNameœ:œdataœ,œidœ:œParseData-2Umhsœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","id":"reactflow__edge-MergeDataComponent-wrxRM{œdataTypeœ:œMergeDataComponentœ,œidœ:œMergeDataComponent-wrxRMœ,œnameœ:œmerged_dataœ,œoutput_typesœ:[œDataœ]}-ParseData-2Umhs{œfieldNameœ:œdataœ,œidœ:œParseData-2Umhsœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"data","id":"ParseData-2Umhs","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"MergeDataComponent","id":"MergeDataComponent-wrxRM","name":"merged_data","output_types":["Data"]}},"selected":false,"className":"","animated":false},{"source":"SplitText-DsAr8","target":"MergeDataComponent-wrxRM","sourceHandle":"{œdataTypeœ:œSplitTextœ,œidœ:œSplitText-DsAr8œ,œnameœ:œchunksœ,œoutput_typesœ:[œDataœ]}","targetHandle":"{œfieldNameœ:œdata_inputsœ,œidœ:œMergeDataComponent-wrxRMœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","id":"reactflow__edge-SplitText-DsAr8{œdataTypeœ:œSplitTextœ,œidœ:œSplitText-DsAr8œ,œnameœ:œchunksœ,œoutput_typesœ:[œDataœ]}-MergeDataComponent-wrxRM{œfieldNameœ:œdata_inputsœ,œidœ:œMergeDataComponent-wrxRMœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"data_inputs","id":"MergeDataComponent-wrxRM","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"SplitText","id":"SplitText-DsAr8","name":"chunks","output_types":["Data"]}},"selected":false,"className":"","animated":false},{"source":"ParseData-2Umhs","target":"Prompt-n1kOC","sourceHandle":"{œdataTypeœ:œParseDataœ,œidœ:œParseData-2Umhsœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œdataœ,œidœ:œPrompt-n1kOCœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","id":"reactflow__edge-ParseData-2Umhs{œdataTypeœ:œParseDataœ,œidœ:œParseData-2Umhsœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-n1kOC{œfieldNameœ:œdataœ,œidœ:œPrompt-n1kOCœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"data","id":"Prompt-n1kOC","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"ParseData","id":"ParseData-2Umhs","name":"text","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"ParseData-U5b53","target":"Prompt-cJv4n","sourceHandle":"{œdataTypeœ:œParseDataœ,œidœ:œParseData-U5b53œ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œthemeœ,œidœ:œPrompt-cJv4nœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","id":"reactflow__edge-ParseData-U5b53{œdataTypeœ:œParseDataœ,œidœ:œParseData-U5b53œ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-cJv4n{œfieldNameœ:œthemeœ,œidœ:œPrompt-cJv4nœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"theme","id":"Prompt-cJv4n","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"ParseData","id":"ParseData-U5b53","name":"text","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"TextInput-5fqFk","target":"Prompt-cJv4n","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-5fqFkœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œadditional_detailsœ,œidœ:œPrompt-cJv4nœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","id":"reactflow__edge-TextInput-5fqFk{œdataTypeœ:œTextInputœ,œidœ:œTextInput-5fqFkœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-cJv4n{œfieldNameœ:œadditional_detailsœ,œidœ:œPrompt-cJv4nœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"additional_details","id":"Prompt-cJv4n","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-5fqFk","name":"text","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"TextInput-zyk18","target":"Prompt-cJv4n","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-zyk18œ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œdataœ,œidœ:œPrompt-cJv4nœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","id":"reactflow__edge-TextInput-zyk18{œdataTypeœ:œTextInputœ,œidœ:œTextInput-zyk18œ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-cJv4n{œfieldNameœ:œdataœ,œidœ:œPrompt-cJv4nœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"data","id":"Prompt-cJv4n","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-zyk18","name":"text","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"TextInput-afqWT","target":"Prompt-cJv4n","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-afqWTœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œnumber_of_speakersœ,œidœ:œPrompt-cJv4nœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","id":"reactflow__edge-TextInput-afqWT{œdataTypeœ:œTextInputœ,œidœ:œTextInput-afqWTœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-cJv4n{œfieldNameœ:œnumber_of_speakersœ,œidœ:œPrompt-cJv4nœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"number_of_speakers","id":"Prompt-cJv4n","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-afqWT","name":"text","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"TextInput-pJsVV","target":"Prompt-cJv4n","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-pJsVVœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œoutput_formatœ,œidœ:œPrompt-cJv4nœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","id":"reactflow__edge-TextInput-pJsVV{œdataTypeœ:œTextInputœ,œidœ:œTextInput-pJsVVœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-cJv4n{œfieldNameœ:œoutput_formatœ,œidœ:œPrompt-cJv4nœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"output_format","id":"Prompt-cJv4n","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-pJsVV","name":"text","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"TextInput-R3tNw","target":"Prompt-cJv4n","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-R3tNwœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œPodcastDurationœ,œidœ:œPrompt-cJv4nœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","id":"reactflow__edge-TextInput-R3tNw{œdataTypeœ:œTextInputœ,œidœ:œTextInput-R3tNwœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-cJv4n{œfieldNameœ:œPodcastDurationœ,œidœ:œPrompt-cJv4nœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"PodcastDuration","id":"Prompt-cJv4n","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-R3tNw","name":"text","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"Prompt-cJv4n","target":"OpenAIModel-0Us0w","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-cJv4nœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-0Us0wœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","id":"reactflow__edge-Prompt-cJv4n{œdataTypeœ:œPromptœ,œidœ:œPrompt-cJv4nœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OpenAIModel-0Us0w{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-0Us0wœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"OpenAIModel-0Us0w","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-cJv4n","name":"prompt","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"OpenAIModel-0Us0w","target":"TextOutput-TXF0h","sourceHandle":"{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-0Us0wœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œTextOutput-TXF0hœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","id":"reactflow__edge-OpenAIModel-0Us0w{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-0Us0wœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-TextOutput-TXF0h{œfieldNameœ:œinput_valueœ,œidœ:œTextOutput-TXF0hœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"TextOutput-TXF0h","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"OpenAIModel","id":"OpenAIModel-0Us0w","name":"text_output","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"ParseData-nK4P9","target":"Prompt-DJXYx","sourceHandle":"{œdataTypeœ:œParseDataœ,œidœ:œParseData-nK4P9œ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œthemeœ,œidœ:œPrompt-DJXYxœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","id":"reactflow__edge-ParseData-nK4P9{œdataTypeœ:œParseDataœ,œidœ:œParseData-nK4P9œ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-DJXYx{œfieldNameœ:œthemeœ,œidœ:œPrompt-DJXYxœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"theme","id":"Prompt-DJXYx","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"ParseData","id":"ParseData-nK4P9","name":"text","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"TextInput-5fqFk","target":"Prompt-DJXYx","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-5fqFkœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œadditional_detailsœ,œidœ:œPrompt-DJXYxœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","id":"reactflow__edge-TextInput-5fqFk{œdataTypeœ:œTextInputœ,œidœ:œTextInput-5fqFkœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-DJXYx{œfieldNameœ:œadditional_detailsœ,œidœ:œPrompt-DJXYxœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"additional_details","id":"Prompt-DJXYx","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-5fqFk","name":"text","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"TextInput-pJsVV","target":"Prompt-DJXYx","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-pJsVVœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œnumber_of_speakersœ,œidœ:œPrompt-DJXYxœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","id":"reactflow__edge-TextInput-pJsVV{œdataTypeœ:œTextInputœ,œidœ:œTextInput-pJsVVœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-DJXYx{œfieldNameœ:œnumber_of_speakersœ,œidœ:œPrompt-DJXYxœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"number_of_speakers","id":"Prompt-DJXYx","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-pJsVV","name":"text","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"TextInput-R3tNw","target":"Prompt-DJXYx","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-R3tNwœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œoutput_formatœ,œidœ:œPrompt-DJXYxœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","id":"reactflow__edge-TextInput-R3tNw{œdataTypeœ:œTextInputœ,œidœ:œTextInput-R3tNwœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-DJXYx{œfieldNameœ:œoutput_formatœ,œidœ:œPrompt-DJXYxœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"output_format","id":"Prompt-DJXYx","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-R3tNw","name":"text","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"Prompt-DJXYx","target":"OpenAIModel-lJEAI","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-DJXYxœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-lJEAIœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","id":"reactflow__edge-Prompt-DJXYx{œdataTypeœ:œPromptœ,œidœ:œPrompt-DJXYxœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OpenAIModel-lJEAI{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-lJEAIœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"OpenAIModel-lJEAI","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-DJXYx","name":"prompt","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"OpenAIModel-lJEAI","target":"TextOutput-qXA05","sourceHandle":"{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-lJEAIœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œTextOutput-qXA05œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","id":"reactflow__edge-OpenAIModel-lJEAI{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-lJEAIœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-TextOutput-qXA05{œfieldNameœ:œinput_valueœ,œidœ:œTextOutput-qXA05œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"TextOutput-qXA05","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"OpenAIModel","id":"OpenAIModel-lJEAI","name":"text_output","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"ParseData-5dSHz","target":"Prompt-vMF3H","sourceHandle":"{œdataTypeœ:œParseDataœ,œidœ:œParseData-5dSHzœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œthemeœ,œidœ:œPrompt-vMF3Hœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","id":"reactflow__edge-ParseData-5dSHz{œdataTypeœ:œParseDataœ,œidœ:œParseData-5dSHzœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-vMF3H{œfieldNameœ:œthemeœ,œidœ:œPrompt-vMF3Hœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"theme","id":"Prompt-vMF3H","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"ParseData","id":"ParseData-5dSHz","name":"text","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"TextInput-5fqFk","target":"Prompt-vMF3H","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-5fqFkœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œadditional_detailsœ,œidœ:œPrompt-vMF3Hœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","id":"reactflow__edge-TextInput-5fqFk{œdataTypeœ:œTextInputœ,œidœ:œTextInput-5fqFkœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-vMF3H{œfieldNameœ:œadditional_detailsœ,œidœ:œPrompt-vMF3Hœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"additional_details","id":"Prompt-vMF3H","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-5fqFk","name":"text","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"TextInput-zyk18","target":"Prompt-DJXYx","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-zyk18œ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œdataœ,œidœ:œPrompt-DJXYxœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","id":"reactflow__edge-TextInput-zyk18{œdataTypeœ:œTextInputœ,œidœ:œTextInput-zyk18œ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-DJXYx{œfieldNameœ:œdataœ,œidœ:œPrompt-DJXYxœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"data","id":"Prompt-DJXYx","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-zyk18","name":"text","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"TextInput-zyk18","target":"Prompt-vMF3H","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-zyk18œ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œdataœ,œidœ:œPrompt-vMF3Hœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","id":"reactflow__edge-TextInput-zyk18{œdataTypeœ:œTextInputœ,œidœ:œTextInput-zyk18œ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-vMF3H{œfieldNameœ:œdataœ,œidœ:œPrompt-vMF3Hœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"data","id":"Prompt-vMF3H","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-zyk18","name":"text","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"TextInput-afqWT","target":"Prompt-vMF3H","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-afqWTœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œnumber_of_speakersœ,œidœ:œPrompt-vMF3Hœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","id":"reactflow__edge-TextInput-afqWT{œdataTypeœ:œTextInputœ,œidœ:œTextInput-afqWTœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-vMF3H{œfieldNameœ:œnumber_of_speakersœ,œidœ:œPrompt-vMF3Hœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"number_of_speakers","id":"Prompt-vMF3H","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-afqWT","name":"text","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"TextInput-pJsVV","target":"Prompt-vMF3H","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-pJsVVœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œoutput_formatœ,œidœ:œPrompt-vMF3Hœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","id":"reactflow__edge-TextInput-pJsVV{œdataTypeœ:œTextInputœ,œidœ:œTextInput-pJsVVœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-vMF3H{œfieldNameœ:œoutput_formatœ,œidœ:œPrompt-vMF3Hœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"output_format","id":"Prompt-vMF3H","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-pJsVV","name":"text","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"TextInput-R3tNw","target":"Prompt-vMF3H","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-R3tNwœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œPodcastDurationœ,œidœ:œPrompt-vMF3Hœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","id":"reactflow__edge-TextInput-R3tNw{œdataTypeœ:œTextInputœ,œidœ:œTextInput-R3tNwœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-vMF3H{œfieldNameœ:œPodcastDurationœ,œidœ:œPrompt-vMF3Hœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"PodcastDuration","id":"Prompt-vMF3H","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-R3tNw","name":"text","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"Prompt-vMF3H","target":"OpenAIModel-WUq0W","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-vMF3Hœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-WUq0Wœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","id":"reactflow__edge-Prompt-vMF3H{œdataTypeœ:œPromptœ,œidœ:œPrompt-vMF3Hœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OpenAIModel-WUq0W{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-WUq0Wœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"OpenAIModel-WUq0W","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-vMF3H","name":"prompt","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"OpenAIModel-WUq0W","target":"TextOutput-R5U9G","sourceHandle":"{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-WUq0Wœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œTextOutput-R5U9Gœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","id":"reactflow__edge-OpenAIModel-WUq0W{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-WUq0Wœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-TextOutput-R5U9G{œfieldNameœ:œinput_valueœ,œidœ:œTextOutput-R5U9Gœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"TextOutput-R5U9G","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"OpenAIModel","id":"OpenAIModel-WUq0W","name":"text_output","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"TextOutput-qXA05","target":"MarkdownDataExtractorComponent-q71Rc","sourceHandle":"{œdataTypeœ:œTextOutputœ,œidœ:œTextOutput-qXA05œ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œMarkdownDataExtractorComponent-q71Rcœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","id":"reactflow__edge-TextOutput-qXA05{œdataTypeœ:œTextOutputœ,œidœ:œTextOutput-qXA05œ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-MarkdownDataExtractorComponent-q71Rc{œfieldNameœ:œinput_valueœ,œidœ:œMarkdownDataExtractorComponent-q71Rcœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"MarkdownDataExtractorComponent-q71Rc","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"TextOutput","id":"TextOutput-qXA05","name":"text","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"TextOutput-R5U9G","target":"MarkdownDataExtractorComponent-T3UnQ","sourceHandle":"{œdataTypeœ:œTextOutputœ,œidœ:œTextOutput-R5U9Gœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œMarkdownDataExtractorComponent-T3UnQœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","id":"reactflow__edge-TextOutput-R5U9G{œdataTypeœ:œTextOutputœ,œidœ:œTextOutput-R5U9Gœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-MarkdownDataExtractorComponent-T3UnQ{œfieldNameœ:œinput_valueœ,œidœ:œMarkdownDataExtractorComponent-T3UnQœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"MarkdownDataExtractorComponent-T3UnQ","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"TextOutput","id":"TextOutput-R5U9G","name":"text","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"Prompt-n1kOC","target":"skeleton-of-thought-fwOVj","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-n1kOCœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œinputœ,œidœ:œskeleton-of-thought-fwOVjœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","id":"reactflow__edge-Prompt-n1kOC{œdataTypeœ:œPromptœ,œidœ:œPrompt-n1kOCœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-skeleton-of-thought-fwOVj{œfieldNameœ:œinputœ,œidœ:œskeleton-of-thought-fwOVjœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input","id":"skeleton-of-thought-fwOVj","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-n1kOC","name":"prompt","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"OpenAIModel-ZQ7eE","target":"skeleton-of-thought-fwOVj","sourceHandle":"{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-ZQ7eEœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}","targetHandle":"{œfieldNameœ:œllmœ,œidœ:œskeleton-of-thought-fwOVjœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}","id":"reactflow__edge-OpenAIModel-ZQ7eE{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-ZQ7eEœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-skeleton-of-thought-fwOVj{œfieldNameœ:œllmœ,œidœ:œskeleton-of-thought-fwOVjœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"llm","id":"skeleton-of-thought-fwOVj","inputTypes":["LanguageModel"],"type":"other"},"sourceHandle":{"dataType":"OpenAIModel","id":"OpenAIModel-ZQ7eE","name":"model_output","output_types":["LanguageModel"]}},"selected":false,"className":"","animated":false},{"source":"TextInput-pJsVV","target":"Prompt-n1kOC","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-pJsVVœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œnumber_of_speakersœ,œidœ:œPrompt-n1kOCœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","id":"reactflow__edge-TextInput-pJsVV{œdataTypeœ:œTextInputœ,œidœ:œTextInput-pJsVVœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-n1kOC{œfieldNameœ:œnumber_of_speakersœ,œidœ:œPrompt-n1kOCœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"number_of_speakers","id":"Prompt-n1kOC","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-pJsVV","name":"text","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"TextInput-R3tNw","target":"Prompt-n1kOC","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-R3tNwœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œPodcastDurationœ,œidœ:œPrompt-n1kOCœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","id":"reactflow__edge-TextInput-R3tNw{œdataTypeœ:œTextInputœ,œidœ:œTextInput-R3tNwœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-n1kOC{œfieldNameœ:œPodcastDurationœ,œidœ:œPrompt-n1kOCœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"PodcastDuration","id":"Prompt-n1kOC","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-R3tNw","name":"text","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"TextOutput-TXF0h","target":"MarkdownDataExtractorComponent-4JVPI","sourceHandle":"{œdataTypeœ:œTextOutputœ,œidœ:œTextOutput-TXF0hœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œMarkdownDataExtractorComponent-4JVPIœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","id":"reactflow__edge-TextOutput-TXF0h{œdataTypeœ:œTextOutputœ,œidœ:œTextOutput-TXF0hœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-MarkdownDataExtractorComponent-4JVPI{œfieldNameœ:œinput_valueœ,œidœ:œMarkdownDataExtractorComponent-4JVPIœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"MarkdownDataExtractorComponent-4JVPI","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"TextOutput","id":"TextOutput-TXF0h","name":"text","output_types":["Message"]}},"selected":false,"className":"","animated":false},{"source":"MarkdownDataExtractorComponent-T3UnQ","target":"MergeDataComponent-evw4H","sourceHandle":"{œdataTypeœ:œMarkdownDataExtractorComponentœ,œidœ:œMarkdownDataExtractorComponent-T3UnQœ,œnameœ:œoutputœ,œoutput_typesœ:[œDataœ]}","targetHandle":"{œfieldNameœ:œdata_inputsœ,œidœ:œMergeDataComponent-evw4Hœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","id":"reactflow__edge-MarkdownDataExtractorComponent-T3UnQ{œdataTypeœ:œMarkdownDataExtractorComponentœ,œidœ:œMarkdownDataExtractorComponent-T3UnQœ,œnameœ:œoutputœ,œoutput_typesœ:[œDataœ]}-MergeDataComponent-evw4H{œfieldNameœ:œdata_inputsœ,œidœ:œMergeDataComponent-evw4Hœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"data_inputs","id":"MergeDataComponent-evw4H","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"MarkdownDataExtractorComponent","id":"MarkdownDataExtractorComponent-T3UnQ","name":"output","output_types":["Data"]}},"selected":false,"className":"","animated":false},{"source":"MarkdownDataExtractorComponent-q71Rc","target":"MergeDataComponent-evw4H","sourceHandle":"{œdataTypeœ:œMarkdownDataExtractorComponentœ,œidœ:œMarkdownDataExtractorComponent-q71Rcœ,œnameœ:œoutputœ,œoutput_typesœ:[œDataœ]}","targetHandle":"{œfieldNameœ:œdata_inputsœ,œidœ:œMergeDataComponent-evw4Hœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","id":"reactflow__edge-MarkdownDataExtractorComponent-q71Rc{œdataTypeœ:œMarkdownDataExtractorComponentœ,œidœ:œMarkdownDataExtractorComponent-q71Rcœ,œnameœ:œoutputœ,œoutput_typesœ:[œDataœ]}-MergeDataComponent-evw4H{œfieldNameœ:œdata_inputsœ,œidœ:œMergeDataComponent-evw4Hœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"data_inputs","id":"MergeDataComponent-evw4H","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"MarkdownDataExtractorComponent","id":"MarkdownDataExtractorComponent-q71Rc","name":"output","output_types":["Data"]}},"selected":false,"className":"","animated":false},{"source":"MarkdownDataExtractorComponent-4JVPI","target":"MergeDataComponent-evw4H","sourceHandle":"{œdataTypeœ:œMarkdownDataExtractorComponentœ,œidœ:œMarkdownDataExtractorComponent-4JVPIœ,œnameœ:œoutputœ,œoutput_typesœ:[œDataœ]}","targetHandle":"{œfieldNameœ:œdata_inputsœ,œidœ:œMergeDataComponent-evw4Hœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","id":"reactflow__edge-MarkdownDataExtractorComponent-4JVPI{œdataTypeœ:œMarkdownDataExtractorComponentœ,œidœ:œMarkdownDataExtractorComponent-4JVPIœ,œnameœ:œoutputœ,œoutput_typesœ:[œDataœ]}-MergeDataComponent-evw4H{œfieldNameœ:œdata_inputsœ,œidœ:œMergeDataComponent-evw4Hœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"data_inputs","id":"MergeDataComponent-evw4H","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"MarkdownDataExtractorComponent","id":"MarkdownDataExtractorComponent-4JVPI","name":"output","output_types":["Data"]}},"selected":false,"className":"","animated":false},{"source":"MergeDataComponent-evw4H","target":"MultiSpeakerAudioGenerator-B3NAE","sourceHandle":"{œdataTypeœ:œMergeDataComponentœ,œidœ:œMergeDataComponent-evw4Hœ,œnameœ:œmerged_dataœ,œoutput_typesœ:[œDataœ]}","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œMultiSpeakerAudioGenerator-B3NAEœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","id":"reactflow__edge-MergeDataComponent-evw4H{œdataTypeœ:œMergeDataComponentœ,œidœ:œMergeDataComponent-evw4Hœ,œnameœ:œmerged_dataœ,œoutput_typesœ:[œDataœ]}-MultiSpeakerAudioGenerator-B3NAE{œfieldNameœ:œinput_valueœ,œidœ:œMultiSpeakerAudioGenerator-B3NAEœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"input_value","id":"MultiSpeakerAudioGenerator-B3NAE","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"MergeDataComponent","id":"MergeDataComponent-evw4H","name":"merged_data","output_types":["Data"]}},"selected":false,"className":"","animated":false},{"source":"skeleton-of-thought-fwOVj","target":"ParseData-U5b53","sourceHandle":"{œdataTypeœ:œskeleton-of-thoughtœ,œidœ:œskeleton-of-thought-fwOVjœ,œnameœ:œoutputœ,œoutput_typesœ:[œDataœ]}","targetHandle":"{œfieldNameœ:œdataœ,œidœ:œParseData-U5b53œ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","id":"reactflow__edge-skeleton-of-thought-fwOVj{œdataTypeœ:œskeleton-of-thoughtœ,œidœ:œskeleton-of-thought-fwOVjœ,œnameœ:œoutputœ,œoutput_typesœ:[œDataœ]}-ParseData-U5b53{œfieldNameœ:œdataœ,œidœ:œParseData-U5b53œ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"data","id":"ParseData-U5b53","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"skeleton-of-thought","id":"skeleton-of-thought-fwOVj","name":"output","output_types":["Data"]}},"selected":false,"className":"","animated":false},{"source":"skeleton-of-thought-fwOVj","target":"ParseData-nK4P9","sourceHandle":"{œdataTypeœ:œskeleton-of-thoughtœ,œidœ:œskeleton-of-thought-fwOVjœ,œnameœ:œoutputœ,œoutput_typesœ:[œDataœ]}","targetHandle":"{œfieldNameœ:œdataœ,œidœ:œParseData-nK4P9œ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","id":"reactflow__edge-skeleton-of-thought-fwOVj{œdataTypeœ:œskeleton-of-thoughtœ,œidœ:œskeleton-of-thought-fwOVjœ,œnameœ:œoutputœ,œoutput_typesœ:[œDataœ]}-ParseData-nK4P9{œfieldNameœ:œdataœ,œidœ:œParseData-nK4P9œ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"data","id":"ParseData-nK4P9","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"skeleton-of-thought","id":"skeleton-of-thought-fwOVj","name":"output","output_types":["Data"]}},"selected":false,"className":"","animated":false},{"source":"skeleton-of-thought-fwOVj","target":"ParseData-5dSHz","sourceHandle":"{œdataTypeœ:œskeleton-of-thoughtœ,œidœ:œskeleton-of-thought-fwOVjœ,œnameœ:œoutputœ,œoutput_typesœ:[œDataœ]}","targetHandle":"{œfieldNameœ:œdataœ,œidœ:œParseData-5dSHzœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","id":"reactflow__edge-skeleton-of-thought-fwOVj{œdataTypeœ:œskeleton-of-thoughtœ,œidœ:œskeleton-of-thought-fwOVjœ,œnameœ:œoutputœ,œoutput_typesœ:[œDataœ]}-ParseData-5dSHz{œfieldNameœ:œdataœ,œidœ:œParseData-5dSHzœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"data","id":"ParseData-5dSHz","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"skeleton-of-thought","id":"skeleton-of-thought-fwOVj","name":"output","output_types":["Data"]}},"selected":false,"className":"","animated":false}],"viewport":{"x":-2864.825503655129,"y":-371.21667155332716,"zoom":0.7269254792661312}},"description":"Smart Chains, Smarter Conversations.","name":"Podcast generator","last_tested_version":"1.0.19.post2","endpoint_name":null,"is_component":false}